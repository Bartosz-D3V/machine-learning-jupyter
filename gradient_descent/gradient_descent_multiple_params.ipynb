{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is an Machine Learning (ML) algorithm that helps finding parameters $\\theta_0$ to $\\theta_n$ to minimize the cost function result for a given function.\n",
    "\n",
    "In the following example, we can calculate the most optimal parameters from $\\theta_0$ to $\\theta_n$ for a linear function expressed with the following formula:\n",
    "\n",
    "\\begin{align}\n",
    "h_\\theta (x_0, x_1, x_2, x_3, ... x_n) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 + \\cdots + \\theta_n x_n\n",
    "\\end{align}\n",
    "\n",
    "We can easily represent the above formula using matrixes multiplication:\n",
    "\n",
    "\\begin{align}\n",
    "h(x_1, x_2, x_3, ... x_n) = \n",
    " \\begin{matrix}\n",
    "  [\\theta_0, \\theta_1, \\theta_2, \\theta_3, ... \\theta_N]\n",
    " \\end{matrix} * \n",
    " \\begin{matrix}\n",
    "  [1 \\\\\n",
    "  x_1 \\\\\n",
    "  x_2 \\\\\n",
    "  x_3 \\\\\n",
    "  ... \\\\\n",
    "  x_n]\n",
    " \\end{matrix}\n",
    " \\ = \\theta^T * x\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "We will reference to the above formula as '**hypothesis**'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first have a look at the data set that we will be working with:\n",
    "\n",
    "| Size (m2) | Num of bedrooms | Num of floors |  Price |\n",
    "|:---------:|:---------------:|:-------------:|:------:|\n",
    "|    641    |        8        |       3       | 700000 |\n",
    "|    300    |        4        |       2       | 560000 |\n",
    "|    350    |        5        |       2       | 500000 |\n",
    "|    180    |        3        |       1       | 250000 |\n",
    "\n",
    "Let's encode those values into arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [1, 481, 8, 3],\n",
    "    [1, 300, 4, 2],\n",
    "    [1, 350, 5, 2],\n",
    "    [1, 180, 3, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and the expected result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([\n",
    "    [700000],\n",
    "    [560000],\n",
    "    [500000],\n",
    "    [250000]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start the actual calculations, we should do **feature standarisation** which is a combination of feature scaling and mean normalisation.\n",
    "The formula is fairly simple:\n",
    "\\begin{align}\n",
    "X_i := \\dfrac{X_i - \\mu_i}{\\sigma_i}\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma_i = STD(X_i)\n",
    "\\end{align}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align}\n",
    "\\mu_i = AVG(X_i)\n",
    "\\end{align}\n",
    "\n",
    "Recalculation formula for X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_x = np.delete(X, 0, 1);\n",
    "mu = np.mean(trimmed_x, 0)\n",
    "si = np.std(trimmed_x, 0)\n",
    "\n",
    "X_norm = (trimmed_x - mu) / si"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to calculate the theta:\n",
    "\n",
    "\\begin{align*} & \\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum\\limits_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)} \\; & \\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_theta(alpha, m, X, Y):\n",
    "    theta = np.empty([4, 1]) # Initialize as empty matrix for now\n",
    "\n",
    "    x_ones = np.c_[np.ones((4,1)), X]\n",
    "\n",
    "    return theta - alpha / m * x_ones.dot((x_ones.dot(theta) - Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = .001\n",
    "m = y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the gradient descent multiple times till convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[503153.77063313],\n",
       "       [-67193.04040986],\n",
       "       [181291.25989958],\n",
       "       [  1967.05110564]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thetas = np.empty([4, 1])\n",
    "for i in range(0, 9000):\n",
    "    thetas = calc_theta(alpha, m, X_norm, y)\n",
    "thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-jupyter-notes",
   "language": "python",
   "name": "machine-learning-jupyter-notes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
